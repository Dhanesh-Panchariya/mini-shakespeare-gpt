# ğŸ§  Mini Shakespeare GPT

A high-performance, *from-scratch* Transformer implemented in **PyTorch**, trained on over **1 million characters** of Shakespearean text. Guided by Andrej Karpathyâ€™s practical walkthrough of *â€œAttention Is All You Needâ€*, this model demonstrates advanced neural-language-model design, endâ€‘toâ€‘end training, and expressive generation capabilities.

---

## ğŸš€ Key Highlights

| Metric                         | Value                                    |
|-------------------------------|-------------------------------------------|
| Corpus Size                   | ~1,000,000 characters (Shakespeare text) |
| Model Architecture            | 6 Transformer layers Ã— 6 attention heads |
| Embedding Dimension           | 384 (total parameters â‰ˆâ€¯1.1 million)     |
| Maximum Context / Block Size  | 256 previous characters                   |
| Dropout Rate                  | 0.2                                       |
| Optimizer & Learning Rate     | AdamW @ 3eâ€‘4                              |
| Training Iterations           | 5,000 mini-batch steps                    |
| Training Time (GPU)           | ~3 minutes per epoch (medium GPU)         |

---

## ğŸ” Features & Academic Credibility

- Deep dive implementation based on the original **Vaswani etÂ al.* â€œAttention Is All You Needâ€** attention mechanisms.
- Heavily influenced and guided by **Andrej Karpathyâ€™s nanoGPT implementation**, but built manually in PyTorch for clarity and learning.
- Implements **positional embeddings**, **masked multi-head self-attention**, **feed-forward networks**, and **LayerNorm + residuals**.
- Achieves **autoregressive character-level generation**â€”generates Shakespeare-style text one character at a time with causal masking.

---

## ğŸ§‘â€ğŸ’» CLI Interface & Usage Guide

All functionality is accessed via command-line flags:

```bash
# Training mode (GPU recommended, but CPU still works)
python mini_shakespeare.py --mode train

# Generation mode with CPU or GPU using pretrained weights
python mini_shakespeare.py --mode generate --prompt "To be, or not to be" --max_new_tokens 150
```

## ğŸ“‚ Repo Structure
```graphql
.
â”œâ”€â”€ mini_shakespeare.py       # Full Transformer + CLI implementation
â”œâ”€â”€ input.txt                 # Raw Shakespeare corpus (~1â€¯MB text)
â”œâ”€â”€ shakespeare_model.pth     # Pretrained weights (~50â€¯MB)
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ Mini_Shakespheare.ipynb   # Notebook version (optional)
```

## ğŸ“– References
- Vaswani, A. et al. â€œAttention Is All You Needâ€ (2017)
- Karpathy, A. â€œnanoGPTâ€ tutorial implementation
